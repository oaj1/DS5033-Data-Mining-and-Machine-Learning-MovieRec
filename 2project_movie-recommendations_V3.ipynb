{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project One: Moive Recommendations\n",
    "\n",
    "We have an exciting project that will further build your ML abilities and end with a  \n",
    "practical model that you may find useful for yourself! \n",
    "\n",
    "We will work through a series of steps:\n",
    "1. Understand the Overarching Task\n",
    "2. Acquire and Load the Data\n",
    "3. Prepare the Data (for ML)\n",
    "4. Selecting and Training an ML Model\n",
    "5. Presenting an Analysis on the Solution\n",
    "\n",
    "Similar to project 1, we will consolidate this notebook into three following steps:\n",
    "\n",
    "* Tasks 1-2 fall will be under Exploratory Data Analysis (EDA).\n",
    "* Tasks 3-4 will involve building a prediction engine for movies.\n",
    "* Finally, we have our analysis in task five.\n",
    "\n",
    "\n",
    "For this project, the data is on movies from 1996 to 2018, with just over 100K ratings across *over 9000* movies.  \n",
    "Details on the dataset are available at https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html\n",
    "\n",
    "* The dataset is attached on the assignment page as a zip.\n",
    "* It is also available for download at https://files.grouplens.org/datasets/movielens/ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:11:37.818457Z",
     "iopub.status.busy": "2024-11-04T19:11:37.818032Z",
     "iopub.status.idle": "2024-11-04T19:11:37.826055Z",
     "shell.execute_reply": "2024-11-04T19:11:37.824859Z",
     "shell.execute_reply.started": "2024-11-04T19:11:37.818407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#install any libraries you may need\n",
    "import pandas as pd # for dataframes from CSV file\n",
    "import numpy as np # for numerical computation\n",
    "import matplotlib.pyplot as plt #for data visualization\n",
    "import seaborn as sns  #for statistical data visualilization \n",
    "from sklearn.model_selection import train_test_split #this will be used to split data into training and testing data, for ML\n",
    "from sklearn.feature_extraction.text import CountVectorizer #For multilabel encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (20%)\n",
    "\n",
    "The task here is to construct a big picture view of the data set.\n",
    "\n",
    "1. Download and load the dataset using pandas. Collect data into a single dataframe (if needed).\n",
    "2. Print statistics - use the head, info, describe functions\n",
    "3. Feature engineer a column for the score using a metric such as weight rating. \n",
    "4. Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#use multiple code blocks to organize your code\n",
    "#EDA \n",
    "\n",
    "#PART 1, download and load datasets using pandas\n",
    "# Load the CSV files\n",
    "# Load the CSV files\n",
    "ratings = pd.read_csv('C:\\\\Users\\\\ollie\\\\OneDrive\\\\Desktop\\\\Data Analyst\\\\DS5033-Data-Mining-and-Machine-Learning Movie Rec\\\\DS5033-Data-Mining-and-Machine-Learning-MovieRec\\\\ratings.csv')\n",
    "tags = pd.read_csv('C:\\\\Users\\\\ollie\\\\OneDrive\\\\Desktop\\\\Data Analyst\\\\DS5033-Data-Mining-and-Machine-Learning Movie Rec\\\\DS5033-Data-Mining-and-Machine-Learning-MovieRec\\\\tags.csv')\n",
    "movies = pd.read_csv('C:\\\\Users\\\\ollie\\\\OneDrive\\\\Desktop\\\\Data Analyst\\\\DS5033-Data-Mining-and-Machine-Learning Movie Rec\\\\DS5033-Data-Mining-and-Machine-Learning-MovieRec\\\\movies.csv')\n",
    "\n",
    "# Display messages and dataset information with clear formatting\n",
    "print(\"Datasets loaded successfully!\")\n",
    "\n",
    "#Now I'm going to merge the df's into a single datafraem\n",
    "# Merge movies and ratings on 'movieId'\n",
    "movies_ratings = pd.merge(movies, ratings, on='movieId', how='inner')\n",
    "\n",
    "# Merge the result with tags on 'movieId'\n",
    "final_df = pd.merge(movies_ratings, tags, on='movieId', how='left')\n",
    "\n",
    "print(\"\\nFinal DataFrame Created!\\n\")\n",
    "\n",
    "\n",
    "print(\"Combined dataset saved to 'final_combined_dataset.csv'\")\n",
    "print(\"\\nFinal DataFrame Created!\\n\")\n",
    "\n",
    "print(\"Let's take a look at some initial data and statistics.\\n\")\n",
    "\n",
    "#Part 2, print statistics\n",
    "\n",
    "# Display the `describe` statistics\n",
    "print(\"Describe:\\n\")\n",
    "#print(final_df.describe(include='all'))  # Include all columns (numeric and non-numeric)\n",
    "print(final_df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the DataFrame's info\n",
    "print(\"Info:\\n\")\n",
    "final_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the first few rows (head) with all columns visible\n",
    "print(\"Head of the DataFrame:\\n\")\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns are visible\n",
    "pd.set_option('display.width', 1000)       # Avoid line wrapping\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA Continued\n",
    "#Part 4,  Feature engineer a column for the score using a metric such as weight rating.\n",
    "\"\"\"\"\"\n",
    "Weighted Score= (v⋅R)+(m⋅C) /(v+m)\n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    "R: The average rating for the item.\n",
    "\n",
    "v: The number of votes for the item.\n",
    "\n",
    "m: A minimum threshold of votes required for consideration (helps reduce noise from low-vote items).\n",
    "\n",
    "C: The average rating across all items.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#find the mean of all movie ratings\n",
    "C = final_df['rating'].mean()\n",
    "\n",
    "# Calculate m: the 80th percentile of the number of ratings per movie\n",
    "m = final_df.groupby('movieId').size().quantile(0.8)\n",
    "\n",
    "# Group by 'movieId' to compute v (number of ratings) and r (average rating)\n",
    "movie_stats = final_df.groupby('movieId').agg(\n",
    "    v=('userId_x', 'count'),      # v: number of ratings\n",
    "    r=('rating', 'mean')         # r: average rating\n",
    ").reset_index()\n",
    "\n",
    "# Add C and m to the movie_stats DataFrame for reference\n",
    "movie_stats['C'] = C\n",
    "movie_stats['m'] = m\n",
    "\n",
    "\n",
    "# Calculate the weighted rating using the formula\n",
    "movie_stats['weighted_rating'] = ((movie_stats['v'] * movie_stats['r']) + (m * C)) / (movie_stats['v'] + m)\n",
    "\n",
    "\n",
    "\n",
    "# Merge the weighted ratings back into the original DataFrame\n",
    "final_df = pd.merge(final_df, movie_stats[['movieId', 'weighted_rating']], on='movieId', how='left')\n",
    "\n",
    "# Display a sample of the final DataFrame with the new weighted_rating column\n",
    "#print(final_df[['movieId', 'title', 'rating', 'weighted_rating']].head())\n",
    "\n",
    "# Let's sort movies by their weighted rating and see the top 10.\n",
    "top_movies = final_df[['movieId', 'title', 'weighted_rating']].drop_duplicates().sort_values(by='weighted_rating', ascending=False)\n",
    "print(\"Top Movies by Weighted Rating:\\n\", top_movies.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA finished, visualize data\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = sns.scatterplot(\n",
    "    data=movie_stats,\n",
    "    x='r',\n",
    "    y='weighted_rating',\n",
    "    size='v',\n",
    "    hue='v',\n",
    "    palette='viridis',\n",
    "    sizes=(20, 200),\n",
    "    alpha=0.7\n",
    "    )\n",
    "plt.title('Relationship Between Average and Weighted Ratings', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Average Rating (r)', fontsize=12)\n",
    "plt.ylabel('Weighted Rating', fontsize=12)\n",
    "plt.colorbar(scatter.collections[0], label='Number of Votes (v)')\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep and Cleaning (10%)\n",
    "\n",
    "Here, you will work to further investigate your features.\n",
    "\n",
    "1. Analyze and drop redundant data\n",
    "2. Remove missing data records\n",
    "3. Make splits of the data based on users (such a number of reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#use multiple code blocks to organize your code\n",
    "#Data Prep and Cleaning\n",
    "\n",
    "#step 1 analyze and drop redundant data\n",
    "final_df.drop_duplicates()\n",
    "\n",
    "# Show rows with NaN values\n",
    "rows_with_na = final_df[final_df.isna().any(axis=1)]\n",
    "print('Number of rows with missing data')\n",
    "print(rows_with_na.count())\n",
    "\n",
    "#step 2 remove missing rows\n",
    "#I''m going to remove these rows. There are 285762 movies, and only 52549 rows with missing values. \\n Thus I''ll drop these rows')\n",
    "final_df = final_df.dropna()\n",
    "print()\n",
    "# Verify after removal\n",
    "print(f\"Rows remaining after dropping missing values: {len(final_df)}\")\n",
    "\n",
    "print()\n",
    "print('Now we have no missing data records, as seen below')\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prep and Cleaning\n",
    "\n",
    "#Step 4, split data based on number of reviews\n",
    "\n",
    "# Count reviews per user\n",
    "user_review_count = final_df.groupby('userId_x').size().reset_index(name='review_count')\n",
    "\n",
    "# Merge the review count back into the original DataFrame\n",
    "final_df = final_df.merge(user_review_count, on='userId_x', how='left')\n",
    "print(final_df.columns)\n",
    "print(user_review_count.head())  # Check the structure of the user_review_count DataFrame\n",
    "\n",
    "\n",
    "# Now, the column from the merge will be named 'review_count_x' and 'review_count_y'\n",
    "# We want to use 'review_count_x' for the count of reviews\n",
    "\n",
    "# Split based on the number of reviews\n",
    "low_activity = final_df[final_df['review_count'] < 10]\n",
    "medium_activity = final_df[(final_df['review_count'] >= 10) & (final_df['review_count'] <= 50)]\n",
    "high_activity = final_df[final_df['review_count'] > 50]\n",
    "\n",
    "print()\n",
    "#Descriptive statistics for each activity gorup\n",
    "print(\"Low activity reviews (less than 10 reviews):\")\n",
    "print(low_activity['review_count'].describe())\n",
    "\n",
    "print('\\n Medium activity reviews (between 10 and 50 reviews)')\n",
    "print (medium_activity['review_count'].describe())\n",
    "\n",
    "print('\\n High activity reviews (More than 50 reviews)')\n",
    "print (high_activity['review_count'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moive Recommendation (40%)\n",
    "\n",
    "Now you will create and test many ML models. The idea is to play with hyperparameters and model types.\n",
    "You may find that some of your data prep needs further tweaking.\n",
    "\n",
    "Generally, you will want to follow the order of steps below (with 1&2 already completed):\n",
    "\n",
    "1. Create a copy of your data.\n",
    "2. Create a train & test split. Generally, 80-20 splits work, but you can play with this.\n",
    "3. Use a model you understand well for prediction.\n",
    "4. Improve your model(s) using cross-validation.\n",
    "\n",
    "Evaluate the model using the top 10 hits from 2015 (ask for the next 3 recommendations based on each of the following):  \n",
    "\n",
    "1. Chi-Raq\n",
    "2. Creed\n",
    "3. Son of Sau\n",
    "4. Carol\n",
    "5. 45 Years\n",
    "6. Brooklyn\n",
    "7. Spotlight\n",
    "8. Anomalisa\n",
    "9. Inside Out\n",
    "10. Mad Max\n",
    "\n",
    "Ranking from: https://www.rogerebert.com/features/the-ten-best-films-of-2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Movie Recommendation\n",
    "\n",
    "\"\"\"\n",
    "note: I don't understand how you want me to evaluate teh model using the top 10 from the above website. Because some of the movies are NOT in the CSV files, i.e. Son of Sau and Carol\n",
    "\n",
    "I'll use a ML model to make predictions from the datasets that I do have access to from the downloading of the original CSV files that I then created final_df with.\n",
    "\n",
    "I'll then ask for 3 recommendations based off of top 10 hits from the df_copy data set\n",
    "\"\"\"\n",
    "#Step 1, create a copy of my data\n",
    "df_copy = final_df.copy()\n",
    "\n",
    "#Step 2, create a train and test split, 80-20 will be used, since that is standard\n",
    "#I think I want to keep the x and y values as \n",
    "# Features and target\n",
    "X = df_copy[['movieId', 'genres', 'userId_x', 'review_count']]\n",
    "y = df_copy['rating']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#count Vectorizer for genres that are split with |\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split('|'))\n",
    "genres_encoded = vectorizer.fit_transform(X['genres'])\n",
    "\n",
    "#convert to Dataframe and concatenate\n",
    "genres_df = pd.DataFrame(genres_encoded.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "X = X.drop(columns=['genres']).reset_index(drop=True)\n",
    "X = pd.concat([X, genres_df], axis=1)\n",
    "\n",
    "#Display the first few rows of the updated feature set\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis (30%):\n",
    "\n",
    "A critical component in science is communicating your results and explaining the reseasoning behind the results.\n",
    "A good presentation here should include the following:  \n",
    "\n",
    "1. An introduction to the dataset, any things we should know (e.g. how it was collected, common errors). \n",
    "2. What did you discover in your EDA? What do you do with missing values, outliers, etc.  \n",
    "3. What kind of distribution is the data? Is there a skew or high concentration of ratings around a genre?  \n",
    "4. What correlations were revealed in the analysis? e.g. any attributes that correlate positively?  \n",
    "5. Feature selection: which feature worked for recommendations and what was noise? How did you determine?\n",
    "\n",
    "The points above are for guidance; you can choose your template and structure.  \n",
    "The idea is to present a short report (no word counts) that is structured. \n",
    "structured, clear, and concise.  \n",
    "You can refer back to your figures and use external links to explain your insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use this markdown cell to write a report\n",
    "\n",
    "[here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission:\n",
    "\n",
    "You need to prepare your ipynb/jupyter notebook for grading.\n",
    "The two main tasks are ensuring all your cell outputs are present and that you convert the notebook to PDF.\n",
    "\n",
    "The instructs will vary slightly based on the platform (collab, kaggle, anaconda, etc).\n",
    "Generally, inside the notebook, you will want to:\n",
    "1. Restart & clear all cell outputs (optional, may detect buggy program control flow)\n",
    "2. Run all (must do; I need to see your code cell outputs!)\n",
    "\n",
    "Next, you need to download the notebook as a PDF. Unfortunately, exporting as PDF is a bit tricky.\n",
    "An easy work around:\n",
    "1. Download the notebook. (all platforms allow the default .ipynb export)\n",
    "2. https://onlineconvertfree.com/convert-format/ipynb-to-pdf/\n",
    "\n",
    "If you are unable to upload as a PDF, submit the .ipynb. Do NOT upload a .py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    "\n",
    "Please see the associated percentage allocations.  \n",
    "In general, ensure your code runs correctly.  Make your the PDF upload includes your code ouputs.  \n",
    "You will be given significant credit for documentation and pseudo-code.\n",
    "\n",
    "For more details, please read the rubric PDF in the assignment files."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
